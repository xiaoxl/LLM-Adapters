{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('LLM-Adapters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA_LICENSE                     \u001b[0m\u001b[01;34mft-training_set\u001b[0m/\n",
      "LICENSE                          generate.py\n",
      "README.md                        hello.py\n",
      "commonsense_evaluate.py          lengths.ipynb\n",
      "crisismmd_data.ipynb             llm.ipynb\n",
      "\u001b[01;34mcrisismmd_datasplit_all\u001b[0m/         math_running_commands\n",
      "\u001b[01;34mdataset\u001b[0m/                         mathqa.py\n",
      "dockerfile                       multi_dataset_eval.py\n",
      "dockertest.bat                   \u001b[01;34mpeft\u001b[0m/\n",
      "evaluate.py                      picture.jpg\n",
      "export_hf_checkpoint.py          pyproject.toml\n",
      "export_state_dict_checkpoint.py  requirements.txt\n",
      "finetune.py\n"
     ]
    }
   ],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n"
     ]
    }
   ],
   "source": [
    "from finetune import train\n",
    "# from evaluate import evalu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "wearetrain = False\n",
    "if wearetrain is True:    \n",
    "    base_model = 'yahma/llama-7b-hf'\n",
    "    data_path = 'math_10k.json' \n",
    "    output_dir = './trained_models/llama-lora' \n",
    "    batch_size = 16 \n",
    "    micro_batch_size = 4 \n",
    "    num_epochs = 3 \n",
    "    learning_rate = 3e-4 \n",
    "    cutoff_len = 256 \n",
    "    val_set_size = 120 \n",
    "    adapter_name = 'lora'\n",
    "\n",
    "    train(base_model=base_model,\n",
    "            data_path=data_path,\n",
    "            output_dir=output_dir,\n",
    "            batch_size=batch_size,\n",
    "            micro_batch_size=micro_batch_size,\n",
    "            num_epochs=num_epochs,\n",
    "            learning_rate=learning_rate,\n",
    "            cutoff_len=cutoff_len,\n",
    "            val_set_size=val_set_size,\n",
    "            adapter_name=adapter_name\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'evaluate' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m adapter \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoRA\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      5\u001b[0m lora_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./trained_models/llama-lora\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 7\u001b[0m \u001b[43mevaluate\u001b[49m(\n\u001b[1;32m      8\u001b[0m     base_model\u001b[38;5;241m=\u001b[39mbase_model,\n\u001b[1;32m      9\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     10\u001b[0m     dataset\u001b[38;5;241m=\u001b[39mdataset,\n\u001b[1;32m     11\u001b[0m     adapter\u001b[38;5;241m=\u001b[39madapter,\n\u001b[1;32m     12\u001b[0m     lora_weights\u001b[38;5;241m=\u001b[39mlora_weights,\n\u001b[1;32m     13\u001b[0m     )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'evaluate' is not defined"
     ]
    }
   ],
   "source": [
    "base_model = 'yahma/llama-7b-hf'\n",
    "model = 'yahma/LLaMA-7B'\n",
    "dataset = 'SVAMP' \n",
    "adapter = 'LoRA'\n",
    "lora_weights = './trained_models/llama-lora'\n",
    "\n",
    "evaluate(\n",
    "    base_model=base_model,\n",
    "    model=model,\n",
    "    dataset=dataset,\n",
    "    adapter=adapter,\n",
    "    lora_weights=lora_weights,\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14384103622589042"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "def kl(p, q):\n",
    "    return p*math.log(p/q)+(1-p)*math.log((1-p)/(1-q))\n",
    "\n",
    "kl(0.5, 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13081203594113697"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kl(0.25, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1438410362258904"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l2=math.log(2)\n",
    "l3=math.log(3)\n",
    "l2-.5*l3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.130812035941137"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ".75*l3-l2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
